# -*- coding: utf-8 -*-
"""ULL_TFM_Source_Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XdX2_SffZy6XQf6AoF1rpCSY2zw4OJxI

**Libraries**
"""

import pandas as pd
import numpy as np
from math import sqrt, pow, exp
import gensim
import nltk
from nltk import word_tokenize ,sent_tokenize
nltk.download('punkt')
from gensim.models.fasttext import FastText
from gensim.models import KeyedVectors
import io
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

from gensim.models import Word2Vec,fasttext,doc2vec

"""**Description & preprocessing data**"""

#loading
datos = pd.read_csv('/content/sample_data/muestra.csv', sep=",")
datos.drop('Unnamed: 0', inplace=True, axis=1) #removing the  column Unnamed
datos.head()

"""Distribution of missing values"""

print("Distribution of missing values for each variable: ")
print(datos.isnull().sum())
print("......................")
print("Registers: ",datos.shape)

"""We add a nvia_count for coutning the number of word copomsing a "nvia" for each address  """

datos['nvia_count'] = datos['nvia'].apply(lambda x: len(str(x).split(" ")))
datos[['nvia','nvia_count']]

datos.nvia.str.split().\
    map(lambda x: len(x)).\
    hist()

"""Most commonly used words on "nvia: nombre de via" for addresses"""

word_cloud = WordCloud(width = 1000,
                       height = 800,
                       colormap = 'Blues',
                       margin = 0,
                       max_words = 200,
                       min_word_length = 4,
                       max_font_size = 120, min_font_size = 15,
                       background_color = "white").generate(" ".join(datos['nvia']))
plt.figure(figsize = (10, 15))
plt.imshow(word_cloud, interpolation = "gaussian")
plt.axis("off")
plt.show()

word_cloud = WordCloud(width = 1000,
                       height = 800,
                       colormap = 'Blues',
                       margin = 0,
                       max_words = 200,
                       min_word_length = 4,
                       max_font_size = 120, min_font_size = 15,
                       background_color = "white").generate(" ".join(datos['tvia']))
plt.figure(figsize = (10, 15))
plt.imshow(word_cloud, interpolation = "gaussian")
plt.axis("off")
plt.show()

#We create a knew "direccion" column to get root of an address by  removing the nommun who is the same
datos['DIRECC']= datos['tvia']+" " + datos['nvia']+ " " + datos['numer'].map(str)
datos[['tvia','nvia','numer','DIRECC']]

#Put adresses in a list format
addresses=datos['DIRECC'].to_list()

#Put uuid_idt in a list format
uuids=datos['uuid_idt'].to_list() #we need this format for the confusion matrix

"""Tokenisation"""

#Before training the model  we have to put adresses dataset  rightformat format
data = []
# iterate through each address
for i in addresses:
    temp = []
    # tokenize the address into words
    for j in word_tokenize(i):
        temp = [t for t in temp if len(t) > 1]
        temp.append(j)

    data.append(temp)
data

"""# Build word2Vec Model"""

#Training  model
w2v_model1=  Word2Vec(data, min_count=1,size= 100,workers=1)

#Loadingg pre-trained model
w2v_model2 = Word2Vec.load("/content/drive/MyDrive/models/complete_model/complete.model")

"""Vectorization of each adresses"""

def vectorize(list_of_docs, model):
    features = []

    for tokens in list_of_docs:
        zero_vector = np.zeros(model.vector_size)
        vectors = []
        for token in tokens:
            if token in model.wv:
                try:
                    vectors.append(model.wv[token])
                except KeyError:
                    continue
        if vectors:
            vectors = np.asarray(vectors)
            avg_vec = vectors.mean(axis=0)
            features.append(avg_vec)
        else:
            features.append(zero_vector)
    return features

"""creating vectors with the Word2vec model 1"""

w2v_model_1_vectors = vectorize(addresses, model=w2v_model1)
print(w2v_model_1_vectors[0],addresses[0])

"""creating vectors with the Word2vec model 2"""

w2v_model_2_vectors = vectorize(addresses, model=w2v_model2)
len(w2v_model_1_vectors[0])

"""**Definition of some usefull functions**

Getting cosinus similarity
"""

def squared_sum(x):
  """ return 3 rounded square rooted value """

  return round(sqrt(sum([a*a for a in x])),3)

def cos_similarity(x,y):
  """ return cosine similarity between two lists """

  numerator = sum(a*b for a,b in zip(x,y))
  denominator = squared_sum(x)*squared_sum(y)
  return round(numerator/float(denominator),3)

def calcul_similarity(vects,adr):
  similarity = []
  for i in range(len(adr)):
      row = []
      for j in range(len(adr)):
          row.append(cos_similarity(vects[i],vects[j]))
      similarity.append(row)
  return similarity

"""Creating Heatmap function"""

def create_heatmap(similarity,adr,cmap = "YlGnBu"):
  labels = [headline for headline in adr]
  df = pd.DataFrame(similarity)
  df.columns = labels
  df.index = labels
  fig, ax = plt.subplots(figsize=(5,5))
  sns.heatmap(df, cmap=cmap)

"""Creation Confusion Matrix Function"""

def MatrixConfusion(vectors,uuid):
  TP=0
  FP=0
  TN=0
  FN=0
  for k in range(len(uuid)):
    row=[]
    for j in range(len(uuid)):
        if(cos_similarity(vectors[k],vectors[j])>0.9 and uuid[k]==uuid[j]):
          TP=TP+1
        if(cos_similarity(vectors[k],vectors[j])>0.9 and uuid[k]!=uuid[j]):
          FP=FP+1
        if(cos_similarity(vectors[k],vectors[j])<0.9 and uuid[k]==uuid[j]):
          FN=FN+1
        if(cos_similarity(vectors[k],vectors[j])<0.9 and uuid[k]!=uuid[j]):
          TN=TN+1
  Accuracy = (TP+TN)/(TP+FP+TN+FN)
  Precision = TP/(TP+FP)
  Recall=TP/(TP+FN)
  F1=(2*TP)/((2*TP)+FP+FN)
  print("****Confusion Matrix******")
  print("Accuracy : ",Accuracy)
  print("Precision : ",Precision)
  print("Recall : ",Recall)

"""**Wor2Vec Results**

**1-Model 1 (trained with address dataset)**

**Heatmap**
"""

create_heatmap(calcul_similarity(w2v_model_1_vectors,addresses),addresses)

"""**Confusion Matrix**"""

MatrixConfusion(w2v_model_1_vectors,uuids)

"""**2- Model 2 (loaded from  spanish Word2vec pretrained models )**

**Heatmap**
"""

create_heatmap(calcul_similarity(w2v_model_2_vectors,addresses),addresses)

"""**Confusion Matrix**"""

MatrixConfusion(w2v_model_2_vectors,uuids)

"""# Build FastText Models"""

#Training Model
fast_Text_model1 = FastText(data,size=100, min_count=1 , workers = 1)

"""Generating addresses as Vectors with model 1"""

fastText_model_1_vectors = vectorize(addresses, model=fast_Text_model1)
len(fastText_model_1_vectors[0])

fastText_model_2 = KeyedVectors.load_word2vec_format("/content/drive/MyDrive/models/fastText/cc.es.300.vec")

fastText_model_2_vectors = vectorize(addresses, model=fastText_model_2)
len(fastText_model_2_vectors[0])

"""**FastText Results**

**1-Model 1 (trained with address dataset)**

**Heatmap**
"""

create_heatmap(calcul_similarity(fastText_model_1_vectors ,addresses),addresses)

"""**Confusion Matrix**"""

MatrixConfusion(fastText_model_1_vectors,uuids)

"""**2-Model 2 (load a fastext spanish pre-trained model)**

**Heatmap**
"""

create_heatmap(calcul_similarity(fastText_model_2_vectors ,addresses),addresses)

"""**Confusion Matrix**"""

MatrixConfusion(fastText_model_2_vectors,uuids)

"""# Build Doc2vec Models"""

#initialise the model
doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=1, epochs=30)

#tag Document
def tagged_document(list_of_list_of_words):
   for i, list_of_words in enumerate(list_of_list_of_words):
      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])
training_data = list(tagged_document(data))

#building vocabulary
doc2vec_model.build_vocab(training_data)
#Train the model
doc2vec_model.train(training_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)

"""Generating address as vectors"""

doc2vec_vectors = [doc2vec_model.infer_vector([word for word in sent]).reshape(1,-1) for sent in addresses]
print(doc2vec_vectors[0],addresses[0])

"""We need to Make some changes to deal with with cos_similarity on doc2vec"""

sim = []
for i in range(len(addresses)):
    row = []
    for j in range(len(addresses)):
        row.append(cos_similarity(doc2vec_vectors[i][0],doc2vec_vectors[j][0]))
    sim.append(row)

def MatrixConfusion_doc(vectors,uuid):
  TP=0
  FP=0
  TN=0
  FN=0
  for k in range(len(uuid)):
    row=[]
    for j in range(len(uuid)):
        if(cos_similarity(vectors[k][0],vectors[j][0])>0.9 and uuid[k]==uuid[j]):
          TP=TP+1
        if(cos_similarity(vectors[k][0],vectors[j][0])>0.9 and uuid[k]!=uuid[j]):
          FP=FP+1
        if(cos_similarity(vectors[k][0],vectors[j][0])<0.9 and uuid[k]==uuid[j]):
          FN=FN+1
        if(cos_similarity(vectors[k][0],vectors[j][0])<0.9 and uuid[k]!=uuid[j]):
          TN=TN+1
  Accuracy = (TP+TN)/(TP+FP+TN+FN)
  Precision = TP/(TP+FP)
  Recall=TP/(TP+FN)
  F1=(2*TP)/((2*TP)+FP+FN)
  print("****Confusion Matrix******")
  print("Accuracy : ",Accuracy)
  print("Precision : ",Precision)
  print("Recall : ",Recall)

"""**Doc2Vec Results**

**Heatmap**
"""

create_heatmap(sim,addresses)

"""**Confusion Matrix**"""

MatrixConfusion_doc(doc2vec_vectors,uuids)

"""# BERT spanish Model

Installing Tranformer
"""

pip install -U sentence-transformers

#Loading Tranformers
from sentence_transformers import SentenceTransformer

"""Loading a Spanish BERT model"""

bert_model = SentenceTransformer('hackathon-pln-es/paraphrase-spanish-distilroberta')

"""generating addresses as vectors"""

bert_vectors = bert_model.encode(addresses)
bert_vectors.shape

"""**BERT results**

**Heatmap**
"""

create_heatmap(calcul_similarity(bert_vectors,addresses),addresses)

"""**Confusion Matrix**"""

MatrixConfusion(bert_vectors,uuids)